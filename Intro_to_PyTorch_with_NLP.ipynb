{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa1e102",
   "metadata": {},
   "source": [
    "# Intro to PyTorch — Hands‑on Notebook (with NLP example)\n",
    "*Generated on: 2025-09-24*\n",
    "\n",
    "This notebook is a practical, end‑to‑end tour of PyTorch, focusing on:\n",
    "- Core tensors, autograd, and modules (`torch`, `torch.nn`, `torch.optim`, `torch.utils.data`)\n",
    "- Building neural networks in multiple styles (Sequential, subclassed `nn.Module`, functional API)\n",
    "- Training pipelines (datasets, dataloaders, training/eval loops, saving/loading)\n",
    "- Useful segments: initialization, regularization, scheduling, mixed precision, `torch.compile` (PyTorch 2.x)\n",
    "- NLP mini‑project: sentiment classification on a tiny in‑notebook dataset using embeddings + RNN/Transformer\n",
    "\n",
    "> **Tip:** Run the cells one by one. If PyTorch is not installed, install it with the cell below (adjust CUDA/CPU as needed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c5e17",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If PyTorch is not installed, uncomment one of these (choose the right one for your machine).\n",
    "# CPU-only (works everywhere):\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# If you have CUDA 12.1:\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Optional utils used later:\n",
    "# !pip install matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec582a",
   "metadata": {},
   "source": [
    "## 2. Imports & Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16aaf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "\n",
    "import torch\n",
    "print('PyTorch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bf467",
   "metadata": {},
   "source": [
    "## 3. Tensors 101\n",
    "Core creation, shapes, dtypes, basic math, broadcasting, indexing, and moving to devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creation\n",
    "x = torch.tensor([[1., 2.],[3., 4.]])\n",
    "a = torch.arange(10)\n",
    "z = torch.zeros((2,3))\n",
    "o = torch.ones((3,2))\n",
    "r = torch.randn((2,2))\n",
    "\n",
    "print('x:', x, x.shape, x.dtype)\n",
    "print('a:', a)\n",
    "print('z:', z)\n",
    "print('o:', o)\n",
    "print('r:', r)\n",
    "\n",
    "# Basic ops & broadcasting\n",
    "b = torch.randn(1,2)\n",
    "print('b:', b)\n",
    "print('x + b:', x + b)\n",
    "print('x @ x.T:', x @ x.T)\n",
    "\n",
    "# Indexing & slicing\n",
    "print('a[2:7]:', a[2:7])\n",
    "print('x[:, 1]:', x[:, 1])\n",
    "\n",
    "# Devices\n",
    "x_gpu = x.to(device)\n",
    "print('x on device:', x_gpu.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5c90a",
   "metadata": {},
   "source": [
    "## 4. Autograd (Automatic Differentiation)\n",
    "Track gradients with `requires_grad=True`, compute loss, and backprop using `.backward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37523c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(3, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 1)\n",
    "\n",
    "# Simple linear model: y_hat = x @ w + b\n",
    "y_hat = x @ w.unsqueeze(1) + b\n",
    "loss = ((y_hat - y)**2).mean()\n",
    "print('Loss:', loss.item())\n",
    "\n",
    "loss.backward()\n",
    "print('w.grad:', w.grad)\n",
    "print('b.grad:', b.grad)\n",
    "\n",
    "# Always zero grads before next backward pass in real training loops\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c2aae",
   "metadata": {},
   "source": [
    "## 5. `torch.nn` Modules\n",
    "Three ways to define models:\n",
    "1. **Sequential API**\n",
    "2. **Subclass `nn.Module`**\n",
    "3. **Functional API** with layers in `forward`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) Sequential\n",
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "print(seq_model)\n",
    "\n",
    "# 2) Subclassing nn.Module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=10, hidden=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, out_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp)\n",
    "\n",
    "# 3) Functional: often similar to subclassing; you call F.* in forward as above.\n",
    "# The MLP already demonstrates functional usage via F.relu in forward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e542720",
   "metadata": {},
   "source": [
    "## 6. Optimizers & A Minimal Training Loop\n",
    "How to wire loss, optimizer, zero‑grad → backward → step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d27eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Dummy regression data\n",
    "X = torch.randn(256, 10)\n",
    "true_w = torch.randn(10, 1)\n",
    "y = X @ true_w + 0.1*torch.randn(256,1)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(10,64), nn.ReLU(), nn.Linear(64,1)).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(X)\n",
    "    loss = criterion(preds, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'Epoch {epoch+1:03d} | loss={loss.item():.4f}')\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mse = criterion(model(X), y).item()\n",
    "print('Final MSE:', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd71d2",
   "metadata": {},
   "source": [
    "## 7. Data Pipeline: `Dataset` & `DataLoader`\n",
    "- Use built‑in datasets or **create your own** by subclassing `torch.utils.data.Dataset`.\n",
    "- Load with `DataLoader` for batching, shuffling, and parallel workers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864405f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, n=500, in_dim=10):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.X = torch.randn(n, in_dim)\n",
    "        self.y = (self.X.sum(dim=1, keepdim=True) > 0).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = ToyDataset(n=1000, in_dim=20)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch_X, batch_y in train_loader:\n",
    "    print(batch_X.shape, batch_y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d44605",
   "metadata": {},
   "source": [
    "## 8. Put It Together: Train a Classifier with DataLoader\n",
    "Includes accuracy computation and evaluation loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim=20, hidden=64, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).float()\n",
    "    return (preds.eq(y).float().mean().item())\n",
    "\n",
    "train_ds = ToyDataset(n=2000, in_dim=20)\n",
    "val_ds   = ToyDataset(n=400,  in_dim=20)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False)\n",
    "\n",
    "model = Classifier(in_dim=20).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_cnt = 0.0, 0.0, 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*Xb.size(0)\n",
    "        total_acc  += accuracy_from_logits(logits, yb)*Xb.size(0)\n",
    "        total_cnt  += Xb.size(0)\n",
    "    tr_loss = total_loss/total_cnt\n",
    "    tr_acc  = total_acc/total_cnt\n",
    "\n",
    "    # Val\n",
    "    model.eval()\n",
    "    val_loss, val_acc, val_cnt = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item()*Xb.size(0)\n",
    "            val_acc  += accuracy_from_logits(logits, yb)*Xb.size(0)\n",
    "            val_cnt  += Xb.size(0)\n",
    "\n",
    "    val_loss /= val_cnt\n",
    "    val_acc  /= val_cnt\n",
    "    print(f'Epoch {epoch+1:02d} | train_loss={tr_loss:.4f} tr_acc={tr_acc:.3f} | val_loss={val_loss:.4f} val_acc={val_acc:.3f}')\n",
    "\n",
    "    best_val = max(best_val, val_acc)\n",
    "print('Best Val Acc:', best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66883fb0",
   "metadata": {},
   "source": [
    "## 9. Saving & Loading Models\n",
    "Use `state_dict` for portability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'classifier_state_dict.pt')\n",
    "\n",
    "# Load (example)\n",
    "loaded = Classifier(in_dim=20).to(device)\n",
    "loaded.load_state_dict(torch.load('classifier_state_dict.pt', map_location=device))\n",
    "loaded.eval()\n",
    "print('Loaded model ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d5604",
   "metadata": {},
   "source": [
    "## 10. Useful Building Blocks\n",
    "- **Initialization**: Xavier/He init\n",
    "- **Regularization**: Weight decay (L2), dropout\n",
    "- **Schedulers**: `StepLR`, `CosineAnnealingLR`, etc.\n",
    "- **Mixed Precision**: `torch.cuda.amp`\n",
    "- **PyTorch 2.x**: `torch.compile` for speedups (if supported)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example: custom init\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "demo = nn.Sequential(nn.Linear(32,64), nn.ReLU(), nn.Linear(64,10))\n",
    "demo.apply(init_weights);\n",
    "\n",
    "# Dropout example\n",
    "drop_model = nn.Sequential(\n",
    "    nn.Linear(32, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "# Schedulers\n",
    "optimizer = optim.Adam(demo.parameters(), lr=1e-3, weight_decay=1e-4)  # weight decay = L2\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# AMP mixed precision (requires CUDA)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# torch.compile (PyTorch 2.x)\n",
    "try:\n",
    "    compiled_demo = torch.compile(demo)  # falls back if not supported\n",
    "except Exception as e:\n",
    "    print('torch.compile not available:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa73ab",
   "metadata": {},
   "source": [
    "## 11. NLP Mini‑Project — Tiny Sentiment Classifier\n",
    "We'll build a minimal text classification pipeline **without external downloads**:\n",
    "\n",
    "**Steps**\n",
    "1. Build a tiny in‑notebook dataset (positive/negative sentences)\n",
    "2. Tokenize (basic), build a vocabulary, numericalize\n",
    "3. Create a `collate_fn` to pad batches\n",
    "4. Model 1: Embedding + Average pooling classifier (fast baseline)\n",
    "5. Model 2: Embedding + LSTM/GRU\n",
    "6. (Optional) Model 3: Tiny Transformer encoder\n",
    "\n",
    "We'll measure accuracy on a simple train/val split. This is **illustrative** (not SOTA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e679e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, random, math, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "random.seed(0); torch.manual_seed(0)\n",
    "\n",
    "# 1) Tiny dataset (toy)\n",
    "pos = [\n",
    "    'i love this movie',\n",
    "    'this film was awesome',\n",
    "    'such a great experience',\n",
    "    'absolutely fantastic acting',\n",
    "    'i really enjoyed it',\n",
    "    'highly recommend this',\n",
    "    'brilliant and heartwarming',\n",
    "    'super fun and engaging',\n",
    "]\n",
    "neg = [\n",
    "    'i hate this movie',\n",
    "    'this film was terrible',\n",
    "    'such a bad experience',\n",
    "    'absolutely awful acting',\n",
    "    'i really disliked it',\n",
    "    'do not recommend this',\n",
    "    'boring and disappointing',\n",
    "    'super dull and messy',\n",
    "]\n",
    "\n",
    "all_text = [(t,1) for t in pos] + [(t,0) for t in neg]\n",
    "random.shuffle(all_text)\n",
    "\n",
    "# 2) Tokenize + vocab\n",
    "def basic_tokenize(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z0-9\\s]+','', s)\n",
    "    return s.strip().split()\n",
    "\n",
    "# build vocab\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for t,_ in all_text:\n",
    "    counter.update(basic_tokenize(t))\n",
    "\n",
    "itos = ['<pad>','<unk>'] + [w for w,cnt in counter.items() if cnt>=1]\n",
    "stoi = {w:i for i,w in enumerate(itos)}\n",
    "\n",
    "def numericalize(tokens):\n",
    "    return [stoi.get(tok, stoi['<unk>']) for tok in tokens]\n",
    "\n",
    "# 3) Dataset with numericalized text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.items = []\n",
    "        for txt, lbl in pairs:\n",
    "            toks = basic_tokenize(txt)\n",
    "            ids = numericalize(toks)\n",
    "            self.items.append((ids, lbl))\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "\n",
    "def pad_collate(batch, pad_idx=0):\n",
    "    # batch: list of (ids, label)\n",
    "    max_len = max(len(ids) for ids,_ in batch)\n",
    "    padded = []\n",
    "    labels = []\n",
    "    for ids, lbl in batch:\n",
    "        arr = ids + [pad_idx]*(max_len - len(ids))\n",
    "        padded.append(arr)\n",
    "        labels.append(lbl)\n",
    "    return torch.tensor(padded), torch.tensor(labels).float().unsqueeze(1)\n",
    "\n",
    "# Split\n",
    "split = int(0.75 * len(all_text))\n",
    "train_pairs = all_text[:split]\n",
    "val_pairs   = all_text[split:]\n",
    "\n",
    "train_ds = TextDataset(train_pairs)\n",
    "val_ds   = TextDataset(val_pairs)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=pad_collate)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "vocab_size = len(itos)\n",
    "pad_idx = stoi['<pad>']\n",
    "vocab_size, pad_idx, itos[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=32, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.fc  = nn.Linear(emb_dim, 1)\n",
    "    def forward(self, x):  # x: [B, T]\n",
    "        e = self.emb(x)    # [B, T, E]\n",
    "        mask = (x != 0).unsqueeze(-1)  # [B, T, 1]\n",
    "        summed = (e * mask).sum(dim=1) # [B, E]\n",
    "        count  = mask.sum(dim=1).clamp(min=1) # [B, 1]\n",
    "        avg    = summed / count\n",
    "        return self.fc(avg)\n",
    "\n",
    "def train_eval(model, train_dl, val_dl, epochs=20, lr=1e-2, device='cpu'):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    def accuracy(logits, y):\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "        return (preds.eq(y).float().mean().item())\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            n = xb.size(0)\n",
    "            total_loss += loss.item()*n\n",
    "            total_acc  += accuracy(logits, yb)*n\n",
    "            total_n    += n\n",
    "        tr_loss = total_loss/total_n\n",
    "        tr_acc  = total_acc/total_n\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_acc, val_n = 0.0, 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                n = xb.size(0)\n",
    "                val_loss += loss.item()*n\n",
    "                val_acc  += accuracy(logits, yb)*n\n",
    "                val_n    += n\n",
    "        val_loss /= val_n\n",
    "        val_acc  /= val_n\n",
    "        if (ep+1)%5==0 or ep==0:\n",
    "            print(f'Epoch {ep+1:02d} | train_loss={tr_loss:.4f} tr_acc={tr_acc:.3f} | val_loss={val_loss:.4f} val_acc={val_acc:.3f}')\n",
    "\n",
    "# Train baseline\n",
    "model = MeanPoolClassifier(vocab_size, emb_dim=32, pad_idx=pad_idx)\n",
    "train_eval(model, train_dl, val_dl, epochs=20, lr=5e-3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, hidden=64, rnn_type='lstm', pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(emb_dim, hidden, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(emb_dim, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        out, h = self.rnn(e)\n",
    "        if isinstance(h, tuple):  # LSTM: (h_n, c_n)\n",
    "            h = h[0]\n",
    "        last = h[-1]  # [B, hidden]\n",
    "        return self.fc(last)\n",
    "\n",
    "rnn_model = RNNClassifier(vocab_size, emb_dim=64, hidden=64, rnn_type='lstm', pad_idx=pad_idx)\n",
    "train_eval(rnn_model, train_dl, val_dl, epochs=20, lr=5e-3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, nhead=4, num_layers=2, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(emb_dim, 1)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)  # [B,T,E]\n",
    "        # Create attention mask where True means \"ignore\"\n",
    "        src_key_padding_mask = (x == self.pad_idx)  # [B,T]\n",
    "        enc = self.encoder(e, src_key_padding_mask=src_key_padding_mask)\n",
    "        # Mean pool over non-pad tokens\n",
    "        mask = (~src_key_padding_mask).unsqueeze(-1)  # [B,T,1]\n",
    "        summed = (enc * mask).sum(dim=1)\n",
    "        count  = mask.sum(dim=1).clamp(min=1)\n",
    "        avg    = summed / count\n",
    "        return self.fc(avg)\n",
    "\n",
    "tx_model = TinyTransformerClassifier(vocab_size, emb_dim=64, nhead=4, num_layers=2, pad_idx=pad_idx)\n",
    "train_eval(tx_model, train_dl, val_dl, epochs=25, lr=3e-3, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be1575",
   "metadata": {},
   "source": [
    "## 12. Inference Utility\n",
    "Test your trained model on custom sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0194617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(model, sentence, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        toks = basic_tokenize(sentence)\n",
    "        ids  = torch.tensor([numericalize(toks)])\n",
    "        logits = model(ids.to(device))\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "        label = 'POSITIVE' if prob >= threshold else 'NEGATIVE'\n",
    "        return {'prob_positive': prob, 'label': label}\n",
    "\n",
    "# Example (after training)\n",
    "print(predict_sentence(tx_model, \"i really loved this fantastic film\"))\n",
    "print(predict_sentence(tx_model, \"this movie was dull and terrible\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce754a",
   "metadata": {},
   "source": [
    "## 13. Exercises (Try on Your Own)\n",
    "1. Replace `nn.ReLU()` with `nn.LeakyReLU()` or `nn.GELU()` and compare accuracy.\n",
    "2. Add **dropout** layers and observe overfitting changes.\n",
    "3. Implement a **character‑level** tokenizer and compare to word‑level.\n",
    "4. Add a **learning rate scheduler** to the NLP training function.\n",
    "5. Use **weight decay** and compare validation performance.\n",
    "6. Try **GRU** instead of LSTM (`rnn_type='gru'`).\n",
    "7. Increase dataset size with more handcrafted sentences; does the transformer help more?\n",
    "8. Add **early stopping** based on validation loss.\n",
    "9. Save the best model’s `state_dict` and reload it for inference.\n",
    "10. If on PyTorch 2.x with CUDA, try wrapping the model with `torch.compile` and compare speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4927b",
   "metadata": {},
   "source": [
    "## 14. Appendix — Quick Cheatsheet\n",
    "- **Tensors**: `torch.tensor`, `torch.arange`, `.to(device)`, `.view()/.reshape()`, `.permute()`\n",
    "- **Autograd**: `requires_grad`, `.backward()`, `with torch.no_grad()`\n",
    "- **Modules**: `nn.Linear`, `nn.Conv2d`, `nn.RNN/LSTM/GRU`, `nn.Embedding`, `nn.Dropout`, `nn.LayerNorm`\n",
    "- **Losses**: `nn.MSELoss`, `nn.CrossEntropyLoss`, `nn.BCEWithLogitsLoss`\n",
    "- **Optim**: `optim.SGD/Adam/AdamW`, `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`\n",
    "- **Data**: `Dataset`, `DataLoader`, `collate_fn`, `num_workers`\n",
    "- **Utilities**: `torch.save`, `torch.load`, `state_dict`\n",
    "- **Advanced**: `torch.cuda.amp` for mixed precision, `torch.compile` (2.x), LR schedulers\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
